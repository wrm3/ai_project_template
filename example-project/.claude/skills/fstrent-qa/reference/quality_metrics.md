# Quality Metrics Guide

## Overview

This guide defines quality metrics for tracking bug patterns, resolution performance, and overall system quality in the fstrent_spec_tasks QA system.

## Bug Metrics

### Bug Discovery Rate
**Definition**: Number of bugs found per development cycle

**Formula**: `Bugs Found / Time Period`

**Targets:**
- Development: 5-10 bugs per sprint (good - catching early)
- Testing: 2-5 bugs per sprint (acceptable)
- Production: <1 bug per sprint (goal)

**Interpretation:**
- High in development: Good (catching early)
- High in testing: Concerning (development quality)
- High in production: Critical (quality issues)

**Example:**
```
Sprint 1: 8 bugs found in development, 3 in testing, 0 in production
Sprint 2: 6 bugs found in development, 2 in testing, 1 in production

Analysis: Sprint 1 better - caught more early, none reached production
```

### Bug Resolution Time
**Definition**: Average time from discovery to resolution

**Formula**: `Sum(Resolution Time) / Number of Bugs`

**Targets:**
- Critical: <24 hours
- High: 1-3 days
- Medium: 3-7 days
- Low: <30 days

**Calculation:**
```
Bug 001: Opened 10/15, Closed 10/15 = 0 days (Critical)
Bug 002: Opened 10/16, Closed 10/18 = 2 days (High)
Bug 003: Opened 10/17, Closed 10/22 = 5 days (Medium)

Average: (0 + 2 + 5) / 3 = 2.3 days
```

**Interpretation:**
- Decreasing trend: Improving process
- Increasing trend: Quality or capacity issues
- Spikes: Complex bugs or resource constraints

### Bug Severity Distribution
**Definition**: Breakdown of bugs by severity level

**Formula**: `(Bugs at Severity / Total Bugs) * 100%`

**Healthy Distribution:**
- Critical: <5%
- High: 10-20%
- Medium: 30-40%
- Low: 40-60%

**Example:**
```
Total Bugs: 50
Critical: 2 (4%)
High: 8 (16%)
Medium: 18 (36%)
Low: 22 (44%)

Analysis: Healthy distribution, most bugs are low severity
```

**Red Flags:**
- Critical >10%: Serious quality issues
- High >30%: Development quality problems
- Low <20%: May be under-reporting minor issues

### Feature Impact Analysis
**Definition**: Which features are most affected by bugs

**Calculation:**
```
Feature A: 12 bugs
Feature B: 8 bugs
Feature C: 3 bugs
Feature D: 2 bugs

Analysis: Feature A needs attention (48% of bugs)
```

**Actions:**
- High bug count: Code review, refactoring, additional testing
- Consistent issues: Architecture problems
- Sudden spike: Recent changes introduced issues

### Regression Rate
**Definition**: Percentage of fixes that introduce new bugs

**Formula**: `(Bugs Caused by Fixes / Total Fixes) * 100%`

**Target**: <5%

**Example:**
```
10 bugs fixed in sprint
1 fix introduced new bug

Regression Rate: (1/10) * 100% = 10%

Analysis: Above target, need better testing
```

**Mitigation:**
- Improve test coverage
- Better code review
- Automated regression testing
- Staged rollouts

## Quality Gates

### Code Review Gate
**Requirement**: All code changes reviewed before merge

**Metrics:**
- Review completion rate: Target 100%
- Average review time: Target <4 hours
- Issues found in review: Track trend

**Pass Criteria:**
- At least one approval
- All comments addressed
- No critical issues

### Testing Gate
**Requirement**: Tests pass before deployment

**Metrics:**
- Test coverage: Target >80%
- Test pass rate: Target 100%
- Test execution time: Monitor for slowness

**Pass Criteria:**
- All unit tests pass
- All integration tests pass
- No critical failures

### Documentation Gate
**Requirement**: Documentation updated with changes

**Metrics:**
- Documentation completeness: Target 100%
- Documentation accuracy: Review quarterly

**Pass Criteria:**
- API docs updated
- README current
- Changelog updated

### Performance Gate
**Requirement**: Performance benchmarks met

**Metrics:**
- Response time: <500ms (API)
- Page load: <2s (frontend)
- Database queries: <100ms

**Pass Criteria:**
- No performance regressions
- Benchmarks met
- Load testing passed

### Security Gate
**Requirement**: No security vulnerabilities

**Metrics:**
- Security scan pass rate: Target 100%
- Vulnerability count: Target 0 critical/high

**Pass Criteria:**
- Security scan passed
- No known vulnerabilities
- Dependencies updated

## Quality Workflows

### Bug Triage Workflow
**Frequency**: Daily for critical/high, weekly for medium/low

**Process:**
1. Review new bugs
2. Assign severity
3. Assign to developer
4. Set priority
5. Estimate effort

**Metrics:**
- Triage time: <24 hours
- Accuracy: >90% correct severity

### Bug Resolution Workflow
**Process:**
1. Investigate (status: Investigating)
2. Fix (status: Fixing)
3. Test (status: Testing)
4. Deploy (status: Closed)

**Metrics:**
- Time in each status
- Bottlenecks identified
- Resolution velocity

### Quality Review Workflow
**Frequency**: Weekly

**Process:**
1. Review bug metrics
2. Identify patterns
3. Address high-impact areas
4. Update processes
5. Document improvements

**Metrics:**
- Issues identified
- Actions taken
- Improvements measured

## Reporting

### Daily Report
```markdown
# Daily Bug Report - 2025-10-19

## New Bugs: 3
- 1 Critical (production outage)
- 1 High (feature broken)
- 1 Medium (UI issue)

## Resolved: 5
- 2 Critical
- 2 High
- 1 Medium

## In Progress: 8
- 0 Critical
- 3 High
- 4 Medium
- 1 Low

## Trends:
- Resolution rate improving
- No new critical bugs in 3 days
```

### Weekly Report
```markdown
# Weekly Quality Report - Week 42

## Bug Metrics:
- Bugs Found: 15 (8 dev, 5 test, 2 prod)
- Bugs Resolved: 18
- Average Resolution: 2.1 days
- Regression Rate: 3%

## Severity Distribution:
- Critical: 2 (13%) ⚠️ Above target
- High: 3 (20%)
- Medium: 6 (40%)
- Low: 4 (27%)

## Feature Impact:
- User Auth: 6 bugs (40%)
- Dashboard: 4 bugs (27%)
- Reports: 3 bugs (20%)
- Settings: 2 bugs (13%)

## Actions:
- Focus testing on User Auth
- Code review for Dashboard changes
- Additional unit tests for Reports
```

### Monthly Report
```markdown
# Monthly Quality Report - October 2025

## Summary:
- Total Bugs: 62
- Resolved: 58 (94%)
- Average Resolution: 2.3 days
- Regression Rate: 4%

## Trends:
- Bug discovery rate decreasing (good)
- Resolution time improving
- Production bugs down 50%

## Quality Gates:
- Code Review: 100% ✅
- Testing: 98% ✅
- Documentation: 95% ✅
- Performance: 100% ✅
- Security: 100% ✅

## Achievements:
- Zero critical bugs in production
- Improved test coverage to 85%
- Reduced average resolution time by 30%

## Focus Areas:
- Continue improving test coverage
- Address User Auth bug concentration
- Maintain low production bug rate
```

## Dashboards

### Real-Time Dashboard
- Open bugs by severity
- Bugs opened today
- Bugs closed today
- Average resolution time (7-day)
- Critical bugs (alert if >0)

### Trend Dashboard
- Bug discovery rate (weekly)
- Resolution time trend (monthly)
- Severity distribution (monthly)
- Feature impact (quarterly)
- Regression rate (monthly)

### Team Dashboard
- Bugs per developer
- Resolution time per developer
- Bug reopening rate
- Code review participation

## Best Practices

### 1. Track Consistently
- Record all bugs in BUGS.md
- Update status promptly
- Close bugs when verified

### 2. Review Regularly
- Daily triage
- Weekly metrics review
- Monthly trend analysis

### 3. Act on Insights
- Address patterns
- Improve processes
- Focus on high-impact areas

### 4. Communicate Clearly
- Share metrics with team
- Celebrate improvements
- Address concerns openly

### 5. Continuous Improvement
- Learn from bugs
- Update processes
- Prevent recurrence

## Summary

Quality metrics provide:
- ✅ Visibility into quality trends
- ✅ Early warning of issues
- ✅ Data for process improvement
- ✅ Accountability and tracking
- ✅ Continuous improvement feedback

Track these metrics consistently to maintain and improve software quality.

